#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Fri Nov  8 00:26:36 2019

@author: gaurav
"""

import pandas as pd
import numpy as np
#!pip install nltk
import nltk

from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
ps=PorterStemmer()


data=pd.read_excel('/home/gaurav/Documents/competition/cik_list.xlsx')
data.head()
###################################
'''reading URL from column'''
##################################
from urllib.request import urlopen
my_files=[]
for link in data['SECFNAME_URL']:   
    with urlopen(link) as response:
        myfile = response.read()
        print(myfile)
        my_files.append(myfile)
####################################
print(my_files)

myfile=myfile.decode("utf-8")


import requests
#url="https://www.sec.gov/Archives/edgar/data/3662/0000950170-98-000413.txt"
r=requests.get(myfile)
text=r.text


from nltk.stem import WordNetLemmatizer
import re
wnl=WordNetLemmatizer()

sentence=nltk.sent_tokenize(myfile)

corpus=[]
for i in range(len(sentence)):
    review= re.sub("[^a-zA-Z]"," ", sentence[i])
    review=review.lower()   
    review=review.split()
    review=[wnl.lemmatize(word) for word in review if not word in set(stopwords.words("english"))]
    review=" ".join(review)
    corpus.append(review)

############################
'''counting of words'''
###########################
    
def word_count(str):
    counts = dict()
    words = str.split()

    for word in words:
        if word in counts:
            counts[word] += 1
        else:
            counts[word] = 1

    return counts

print( word_count(corpus[0]))


#################################
'''Total word count'''
################################# 
for new_corpus in corpus:
    new_corpus=print( word_count(new_corpus))

#################################
'''Bag of words'''
#################################

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.ensemble import RandomForestClassifier
rfr=RandomForestClassifier(n_estimators=200,criterion='entropy')
cv=CountVectorizer(ngram_range=(2,2))
cv.fit_transform(corpus).toarray()


import seaborn as sns
import matplotlib.pyplot as plt


plt.hist(data['FYRMO'])

from sklearn.preprocessing import MinMaxScaler
ss=MinMaxScaler(feature_range=(-1,1),copy=True)
ss.fit_transform(data['FYRMO'])














    
    